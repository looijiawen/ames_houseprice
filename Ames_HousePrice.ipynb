{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83f9add-5200-43a2-b2bb-667859a8d362",
   "metadata": {},
   "source": [
    "## <span style=\"color:coral\"> Predicting Ames Housing Prices with Regression\n",
    "    \n",
    "<span style=\"color:teal\">**Background** </span> <br> \n",
    "\n",
    "House prices predictions are useful for *homebuyers* (primary stakeholder) plan their purchase for an ideal house and finances to buy a house. Being able to identify features which the value of the house is sensitive to, they may be in a better position to negotiate price. In addition, house price predictions are also beneficial for *homeowners* (secondary stakeholder) decide how they may want to renovate their houses to improve its marketability and sale price. \n",
    "\n",
    "<span style=\"color:teal\"> **Problem Statement** </span> <br> \n",
    "    \n",
    "- How may homebuyers in Ames identify houses which have great appreciation value? <br>\n",
    "- How may homesellers in Ames improve the values of their house? <br>\n",
    "\n",
    "\n",
    "<span style=\"color:teal\"> **Objective** <br> \n",
    "\n",
    "With a [Ames Housing Dataset](https://www.kaggle.com/c/dsi-us-11-project-2-regression-challenge/data) from Kaggle, which consists of 2051 sold houses in Ames, where each of these houses have 80 features that we will leverage upon to find patterns and correlations, we want to -</span>\n",
    "    \n",
    "\n",
    "    \n",
    "- **identify features** which significantly affect housing prices\n",
    "- **accurately** predict housing prices using those features\n",
    "- build a **generalizable^** regression model to accurately predict housing prices <br>(i.e. ^model should be able to adapt to different sets of test data, and predict housing prices with a decent accuracy)\n",
    "- identify features of a house which affects its value\n",
    "\n",
    "\n",
    "<span style=\"color:teal\"> **Evaluation** </span> of model performance by -\n",
    "    \n",
    "    \n",
    "1. coefficient of discrimination ($R^2$) \n",
    "2. root-mean-squared-error (RMSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95ec2a",
   "metadata": {},
   "source": [
    "### <span style=\"color:royalblue\"> **1. Importing the Libraries & Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b26b539d-ed08-4baf-997d-a77f08366a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2051, 81)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV, Lasso, Ridge, ElasticNet\n",
    "\n",
    "raw_df = pd.read_csv(\"./datasets/train.csv\")\n",
    "test_df = pd.read_csv(\"./datasets/test.csv\")\n",
    "\n",
    "print(raw_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcec7dc",
   "metadata": {},
   "source": [
    "### <span style=\"color:royalblue\"> **2. Understanding the Dataset**\n",
    "<span style=\"color:royalblue\"> **2a. Data Inspection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d47a56-6e70-4d1c-a992-cfb58c88da39",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id\n",
      "PID\n",
      "MS SubClass\n",
      "MS Zoning\n",
      "Lot Frontage\n",
      "Lot Area\n",
      "Street\n",
      "Alley\n",
      "Lot Shape\n",
      "Land Contour\n",
      "Utilities\n",
      "Lot Config\n",
      "Land Slope\n",
      "Neighborhood\n",
      "Condition 1\n",
      "Condition 2\n",
      "Bldg Type\n",
      "House Style\n",
      "Overall Qual\n",
      "Overall Cond\n",
      "Year Built\n",
      "Year Remod/Add\n",
      "Roof Style\n",
      "Roof Matl\n",
      "Exterior 1st\n",
      "Exterior 2nd\n",
      "Mas Vnr Type\n",
      "Mas Vnr Area\n",
      "Exter Qual\n",
      "Exter Cond\n",
      "Foundation\n",
      "Bsmt Qual\n",
      "Bsmt Cond\n",
      "Bsmt Exposure\n",
      "BsmtFin Type 1\n",
      "BsmtFin SF 1\n",
      "BsmtFin Type 2\n",
      "BsmtFin SF 2\n",
      "Bsmt Unf SF\n",
      "Total Bsmt SF\n",
      "Heating\n",
      "Heating QC\n",
      "Central Air\n",
      "Electrical\n",
      "1st Flr SF\n",
      "2nd Flr SF\n",
      "Low Qual Fin SF\n",
      "Gr Liv Area\n",
      "Bsmt Full Bath\n",
      "Bsmt Half Bath\n",
      "Full Bath\n",
      "Half Bath\n",
      "Bedroom AbvGr\n",
      "Kitchen AbvGr\n",
      "Kitchen Qual\n",
      "TotRms AbvGrd\n",
      "Functional\n",
      "Fireplaces\n",
      "Fireplace Qu\n",
      "Garage Type\n",
      "Garage Yr Blt\n",
      "Garage Finish\n",
      "Garage Cars\n",
      "Garage Area\n",
      "Garage Qual\n",
      "Garage Cond\n",
      "Paved Drive\n",
      "Wood Deck SF\n",
      "Open Porch SF\n",
      "Enclosed Porch\n",
      "3Ssn Porch\n",
      "Screen Porch\n",
      "Pool Area\n",
      "Pool QC\n",
      "Fence\n",
      "Misc Feature\n",
      "Misc Val\n",
      "Mo Sold\n",
      "Yr Sold\n",
      "Sale Type\n",
      "SalePrice\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print (i) for i in raw_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde95782",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2051 entries, 0 to 2050\n",
      "Data columns (total 81 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Id               2051 non-null   int64  \n",
      " 1   PID              2051 non-null   int64  \n",
      " 2   MS SubClass      2051 non-null   int64  \n",
      " 3   MS Zoning        2051 non-null   object \n",
      " 4   Lot Frontage     1721 non-null   float64\n",
      " 5   Lot Area         2051 non-null   int64  \n",
      " 6   Street           2051 non-null   object \n",
      " 7   Alley            140 non-null    object \n",
      " 8   Lot Shape        2051 non-null   object \n",
      " 9   Land Contour     2051 non-null   object \n",
      " 10  Utilities        2051 non-null   object \n",
      " 11  Lot Config       2051 non-null   object \n",
      " 12  Land Slope       2051 non-null   object \n",
      " 13  Neighborhood     2051 non-null   object \n",
      " 14  Condition 1      2051 non-null   object \n",
      " 15  Condition 2      2051 non-null   object \n",
      " 16  Bldg Type        2051 non-null   object \n",
      " 17  House Style      2051 non-null   object \n",
      " 18  Overall Qual     2051 non-null   int64  \n",
      " 19  Overall Cond     2051 non-null   int64  \n",
      " 20  Year Built       2051 non-null   int64  \n",
      " 21  Year Remod/Add   2051 non-null   int64  \n",
      " 22  Roof Style       2051 non-null   object \n",
      " 23  Roof Matl        2051 non-null   object \n",
      " 24  Exterior 1st     2051 non-null   object \n",
      " 25  Exterior 2nd     2051 non-null   object \n",
      " 26  Mas Vnr Type     2029 non-null   object \n",
      " 27  Mas Vnr Area     2029 non-null   float64\n",
      " 28  Exter Qual       2051 non-null   object \n",
      " 29  Exter Cond       2051 non-null   object \n",
      " 30  Foundation       2051 non-null   object \n",
      " 31  Bsmt Qual        1996 non-null   object \n",
      " 32  Bsmt Cond        1996 non-null   object \n",
      " 33  Bsmt Exposure    1993 non-null   object \n",
      " 34  BsmtFin Type 1   1996 non-null   object \n",
      " 35  BsmtFin SF 1     2050 non-null   float64\n",
      " 36  BsmtFin Type 2   1995 non-null   object \n",
      " 37  BsmtFin SF 2     2050 non-null   float64\n",
      " 38  Bsmt Unf SF      2050 non-null   float64\n",
      " 39  Total Bsmt SF    2050 non-null   float64\n",
      " 40  Heating          2051 non-null   object \n",
      " 41  Heating QC       2051 non-null   object \n",
      " 42  Central Air      2051 non-null   object \n",
      " 43  Electrical       2051 non-null   object \n",
      " 44  1st Flr SF       2051 non-null   int64  \n",
      " 45  2nd Flr SF       2051 non-null   int64  \n",
      " 46  Low Qual Fin SF  2051 non-null   int64  \n",
      " 47  Gr Liv Area      2051 non-null   int64  \n",
      " 48  Bsmt Full Bath   2049 non-null   float64\n",
      " 49  Bsmt Half Bath   2049 non-null   float64\n",
      " 50  Full Bath        2051 non-null   int64  \n",
      " 51  Half Bath        2051 non-null   int64  \n",
      " 52  Bedroom AbvGr    2051 non-null   int64  \n",
      " 53  Kitchen AbvGr    2051 non-null   int64  \n",
      " 54  Kitchen Qual     2051 non-null   object \n",
      " 55  TotRms AbvGrd    2051 non-null   int64  \n",
      " 56  Functional       2051 non-null   object \n",
      " 57  Fireplaces       2051 non-null   int64  \n",
      " 58  Fireplace Qu     1051 non-null   object \n",
      " 59  Garage Type      1938 non-null   object \n",
      " 60  Garage Yr Blt    1937 non-null   float64\n",
      " 61  Garage Finish    1937 non-null   object \n",
      " 62  Garage Cars      2050 non-null   float64\n",
      " 63  Garage Area      2050 non-null   float64\n",
      " 64  Garage Qual      1937 non-null   object \n",
      " 65  Garage Cond      1937 non-null   object \n",
      " 66  Paved Drive      2051 non-null   object \n",
      " 67  Wood Deck SF     2051 non-null   int64  \n",
      " 68  Open Porch SF    2051 non-null   int64  \n",
      " 69  Enclosed Porch   2051 non-null   int64  \n",
      " 70  3Ssn Porch       2051 non-null   int64  \n",
      " 71  Screen Porch     2051 non-null   int64  \n",
      " 72  Pool Area        2051 non-null   int64  \n",
      " 73  Pool QC          9 non-null      object \n",
      " 74  Fence            400 non-null    object \n",
      " 75  Misc Feature     65 non-null     object \n",
      " 76  Misc Val         2051 non-null   int64  \n",
      " 77  Mo Sold          2051 non-null   int64  \n",
      " 78  Yr Sold          2051 non-null   int64  \n",
      " 79  Sale Type        2051 non-null   object \n",
      " 80  SalePrice        2051 non-null   int64  \n",
      "dtypes: float64(11), int64(28), object(42)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2db761c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ab545e235235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78822d-f8d8-4616-9cf8-ca3a85043f64",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dictionary showing the values for each categorial/nominal columns\n",
    "\n",
    "lst1 = []\n",
    "lst2 = []\n",
    "\n",
    "for i in raw_df.columns:\n",
    "    if raw_df[i].dtype == object and len(list(raw_df[i].value_counts().index))<10:\n",
    "        lst1.append(i)\n",
    "        lst2.append(list(raw_df[i].value_counts().index))\n",
    "        col_dict = zip(lst1,lst2)\n",
    "        \n",
    "raw_df_map = {k: v for k, v in col_dict}\n",
    "raw_df_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting our target variable (SalePrice) at position 0 for easier reference\n",
    "df = raw_df\n",
    "df.set_index('Id', inplace=True)\n",
    "\n",
    "sp_col = df.pop(\"SalePrice\")\n",
    "\n",
    "df.insert(0, 'SalePrice', sp_col)\n",
    "df.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2402ca",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **2b. Our Baseline Model** </span><br>\n",
    "We will create a baseline prediction result to use it as a point for comparison for the model we will create later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e559575",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = raw_df.fillna(0)\n",
    "\n",
    "features = [i for i in raw_df._get_numeric_data().columns if i !='SalePrice']\n",
    "features\n",
    "base_X = raw_df[features]\n",
    "base_y = raw_df['SalePrice']\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(base_X, base_y)\n",
    "base_y_pred = lr.predict(base_X)\n",
    "\n",
    "print(f'Baseline r2 score: {lr.score(base_X, base_y)}')\n",
    "print(f'Baseline RMSE: {np.sqrt(mean_squared_error(base_y, base_y_pred))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc62504",
   "metadata": {},
   "source": [
    "### <span style=\"color:royalblue\"> **3. Data Cleaning**\n",
    "<span style=\"color:royalblue\"> **3a. Dropping Columns (no relation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83556d7",
   "metadata": {},
   "source": [
    "We want to delete the columns have no relation to the sale price, namely: \n",
    "1. `PID` (Parcel identification number) <br>\n",
    "   *Since PID is a just a unique code that has no meaning in the context of our house price prediction, we will proceed to drop it.*\n",
    "\n",
    "2. `Sale Type` (Type of sale) <br>\n",
    "    *For Sale Type, we will check if it has indeed no impact on SalePrice*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Sale Type VS SalePrice')\n",
    "sns.scatterplot(data=df, x='Sale Type', y='SalePrice');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31793375-bf79-4298-94df-7692b3eea5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No relationship, we can conclude that type of sale will not have any impact of SalePrice\n",
    "# Proceed to drop Parcel ID & Type of sale \n",
    "df.drop(['PID', 'Sale Type'], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61690201",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3b. Replacing \"Year\" Columns**\n",
    "    \n",
    "Homebuyers will be more interested in the age of the house they are buying, rather than the year it was built/remodelled. Age of house will have more direct influence on the house value, as compared to the year it was built. <br>\n",
    "Instead of using the year a house was built, we will take the difference between the year built and year sold, to find the age of house (`AgeSold`), at the point of in time when it was sold. The same logic applies for date of remodeling or addition, hence, we have replaced it with Age Remodeled (`AgeRemod`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180f020-b268-4dd8-a858-ad2e397c5589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(regex=r'(Year|Yr)').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23795ece-a487-453a-811f-d96a7036f500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New columns: (i) Age built, (ii) Age remodelled, of the house (in years) at the point when it was sold\n",
    "df['AgeSold'] = df['Yr Sold'] - df['Year Built']\n",
    "df['AgeRemod'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "\n",
    "df[df['AgeSold']<0] = 0\n",
    "df[df['AgeRemod']<0] = 0\n",
    "\n",
    "# Now we no longer need the Year Built and Remodelled\n",
    "df.drop(labels=['Year Built','Year Remod/Add'], axis=1, inplace=True)\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ea11b",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3c. Null Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead45985-49cf-4c45-99e7-333ffaa33787",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_col = pd.DataFrame(df.isnull().sum().sort_values(ascending=False),columns=['null_count'])\n",
    "num_col_null = null_col[null_col['null_count']!= 0].shape[0]\n",
    "print(f'{num_col_null} Columns with Null Values: \\n')\n",
    "null_col[null_col['null_count']!= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be205e56-4731-4379-98e1-fc9da3a0b991",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Categorial Columns in Dataset: \\n')\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == object:\n",
    "        print(i)\n",
    "        print(df[i].unique())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ccf79d",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3c(i). Null Values in Categorial Columns** </span><br>\n",
    "\n",
    "- $\\forall$ Columns with null because $\\not\\exists$ categorical features (or not present) → imputed with a standard string `NA`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742bbb1b-3153-46e1-af6a-7357282b760f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    if df[i].dtypes == object:\n",
    "        df[i].replace(np.nan, \"NA\",inplace=True)\n",
    "    #elif df[i].dtypes == 'int64' or df[i].dtypes == float:\n",
    "        #df[i].replace(np.nan, 0,inplace=True)\n",
    "\n",
    "null_col = pd.DataFrame(df.isnull().sum().sort_values(ascending=False),columns=['null_count'])\n",
    "num_col_null = null_col[null_col['null_count']!= 0].shape[0]\n",
    "num_col_null = null_col[null_col['null_count']!= 0].shape[0]\n",
    "print(f'After Cleaning Caterogorial Null Columns, {num_col_null} Columns with Null Values: \\n')\n",
    "null_col[null_col['null_count']!= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d524bfa7",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3c(ii). Null Values in Lot Frontage** </span><br>\n",
    "Lot Frontage: Linear feet of street connected to property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ece2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_lot_frontage_df = df[df['Lot Frontage'].isnull() == False] # Temporarily exclude null in lot frontage\n",
    "print(f'Correlation Coeff of Lot Frontage and SalePrice:')\n",
    "np.corrcoef(non_null_lot_frontage_df['Lot Frontage'], non_null_lot_frontage_df[\"SalePrice\"])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd5327-332e-4fa8-8717-de8c149626f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not much correlation (0.34) between non-null Lot Frontage and SalePrice, we remove Lot Frontage Column\n",
    "df.drop(labels=['Lot Frontage'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7100e68",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3c(iii). Null Values related to Basement** </span><br>\n",
    "- $\\forall$ Columns with null because $\\not\\exists$ basement, which we have imputed with a standard string `NA` previously <br>\n",
    "- Related numerical features → imputed with 0<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb792a-7895-4395-9f67-c1aa4794d875",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleaning Basement Columns\n",
    "df_bsmt = df.filter(regex= 'Bsmt')\n",
    "\n",
    "df.loc[df_bsmt['Bsmt Qual']=='NA','Bsmt Full Bath'] = 0\n",
    "df.loc[df_bsmt['Bsmt Qual']=='NA','Bsmt Half Bath'] = 0\n",
    "df.loc[df_bsmt['Bsmt Qual']=='NA','BsmtFin SF 1'] = 0\n",
    "df.loc[df_bsmt['Bsmt Qual']=='NA','BsmtFin SF 2'] = 0\n",
    "df.loc[df_bsmt['Bsmt Qual']=='NA','Bsmt Unf SF'] = 0\n",
    "df.loc[df_bsmt['Bsmt Qual']=='NA','Total Bsmt SF'] = 0\n",
    "\n",
    "df_bsmt = df.filter(regex= 'Bsmt')\n",
    "df_bsmt.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8e0b6",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3c(iv). Null Values related to Garage** </span><br>\n",
    "- $\\forall$ Columns with null because $\\not\\exists$ basement, which we have imputed with a standard string `NA` previously <br>\n",
    "- Related numerical features → imputed with 0<br>\n",
    "- Related categorial features → imputed with `NA`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc15bb-9964-498c-b9fe-1f618d4a4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Garage Columns\n",
    "df_gar = df.filter(regex= 'Garage')\n",
    "df_gar['Garage Finish'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24af74-d63a-4708-94bc-11801c829543",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No garage => 'Garage Yr Blt will be 0\n",
    "df.loc[df['Garage Finish']=='NA','Garage Yr Blt'] = 0\n",
    "df_gar = df.filter(regex= 'Garage')\n",
    "df_gar_null = df_gar[df_gar['Garage Area'].isnull()]\n",
    "df_gar_null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00bc4d",
   "metadata": {},
   "source": [
    "We realise there is still a house $\\exists$ garage, but its other related features are 0 or null. <br>\n",
    "Since it is only a single observation, we may either i. drop row or ii. impute null - here, shall do the latter in hopes for brownie points\n",
    "- Related numerical features → imputed with `mean`\n",
    "- Related categorial features → imputed with `mode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(df_gar_null.index):\n",
    "    print(i)\n",
    "    df.loc[i , 'Garage Yr Blt'] = round(df_gar['Garage Yr Blt'].mean(),0)\n",
    "    df.loc[i , 'Garage Cars'] = round(df_gar['Garage Cars'].mean(),0)\n",
    "    df.loc[i , 'Garage Area'] = round(df_gar['Garage Area'].mean(),0)\n",
    "    df.loc[i , 'Garage Finish'] = df_gar['Garage Finish'].mode()[0]\n",
    "    df.loc[i , 'Garage Qual'] = df_gar['Garage Qual'].mode()[0]\n",
    "    df.loc[i , 'Garage Cond'] = df_gar['Garage Cond'].mode()[0]\n",
    "    df_gar = df.filter(regex= 'Garage')\n",
    "df_gar.loc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c787f-5768-4b23-956b-759686d52e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gar = df.filter(regex= 'Garage')\n",
    "df_gar['Garage Finish'].value_counts()\n",
    "df_gar.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6abf8be",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3c(v). Null Values related to Masonry Veneer Type & Area** </span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcc5a2-dd93-4bd1-8c22-2a12cc74494b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Blank Masonry Veneer Area\n",
    "df['Mas Vnr Type'].replace('NA',None, inplace=True)\n",
    "df['Mas Vnr Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7764a7c-a856-463a-9a47-8233b3555aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For non-None Masonry Veneer Type -> replace NaN Masonry Veneer Area with mean area of the same Masonry Veneer Type\n",
    "df.loc[df['Mas Vnr Type']=='None','Mas Vnr Area'] = 0\n",
    "\n",
    "mean_mas_vnr_brk = round(df.loc[df['Mas Vnr Type']=='BrkFace','Mas Vnr Area'].mean(),0)\n",
    "mean_mas_vnr_stone = round(df.loc[df['Mas Vnr Type']=='Stone','Mas Vnr Area'].mean(),0)\n",
    "\n",
    "df.loc[(df['Mas Vnr Type']=='BrkFace') & (df['Mas Vnr Area'].isnull()),'Mas Vnr Area'] = mean_mas_vnr_brk\n",
    "df.loc[(df['Mas Vnr Type']=='Stone') & (df['Mas Vnr Area'].isnull()),'Mas Vnr Area'] = mean_mas_vnr_stone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c55a14-559a-4d27-a945-58324129569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_col = pd.DataFrame(df.isnull().sum().sort_values(ascending=False),columns=['null_count'])\n",
    "num_col_null = null_col[null_col['null_count']!= 0].shape[0]\n",
    "print(f'{num_col_null} Columns with Null Values: \\n')\n",
    "null_col[null_col['null_count']!= 0]\n",
    "# No more columns with Null values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cf0c7",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3d. Columns with Overwhelmingly Single Value** </span><br>\n",
    "When most of the values in a column is a single value, there is not much variation displayed by the predictor. In other words, these columns have near-zero variance, and they are less likely to contain valuable predictive information. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be090c-9ae3-4a19-bce5-6c26d4c20def",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Value Counts of Categorial and Discrete Columns in Dataset: \\n')\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == object or df[i].nunique() < 100:\n",
    "        print(i)\n",
    "        print(df[i].value_counts())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69c444-c543-4331-a555-2c1299a9402c",
   "metadata": {},
   "source": [
    "As we can see in the table below, the values in these columns are dominated by a single value or category (which is more than 96% of the data). As for the rest of the non-dominating values, they occur infrequently in the data. <br>\n",
    "We will drop these columnns predominantly grouped under a single category or value.\n",
    "\n",
    "| Columnn to Drop | Value/Category | Count | Other(s) Count |\n",
    "| --- | --- | --- | --- |\n",
    "| Street | Pave| 2044 | 7 |\n",
    "| Alley | NA | 1909 | 142 |\n",
    "| Utilities | AllPub | 2049 | 2 |\n",
    "| Condition 2 | Norm | 2025 | 26 |\n",
    "| Roof Matl | CompShg | 2025 | 26 |\n",
    "| Low Qual Fin SF | 0 | 2018 | 33 |\n",
    "| Heating | GasA | 2018 | 33 |\n",
    "| Pool QC | NA | 2042 | 9 |\n",
    "| Misc Feature | NA | 1986 | 65 |\n",
    "| Misc Val | 0 | 1987 | 64 |\n",
    "| 3Ssn Porch | 0 | 2025 | 26 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa37b98-9326-4dee-a9d0-eb9c6051053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Street','Alley','Utilities', 'Condition 2', 'Roof Matl', 'Low Qual Fin SF', \n",
    "         'Heating', 'Pool QC', 'Pool Area', 'Misc Feature', '3Ssn Porch','Misc Val'], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c66e0f",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3e. Observations with outlying SalePrice** </span><br>\n",
    "The presence of outliers in our dataset can result in a poor fit and lower predictive modeling performance. We want to identify and remove these outliers to improve our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a4a2c-7b63-42fa-b59f-4130f68e0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df['SalePrice']);\n",
    "# Largely skewed to the left\n",
    "# Seems that there are some possible outliers at the tail end of SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c64907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "saleprice_99_percentile = np.quantile(df['SalePrice'],0.99)\n",
    "saleprice_99_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889638a-8f03-4881-856e-321a98b48e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column withh SalePrice = 0\n",
    "df.drop(df.loc[df['SalePrice']==0].index, inplace=True)\n",
    "# Remove Outliers - Dropping the houses with SalePrice > its 99th percentile\n",
    "df = df[df['SalePrice']<=saleprice_99_percentile]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df['SalePrice']);\n",
    "# Now our SalePrice looks less skewed -> more normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4891964",
   "metadata": {},
   "source": [
    "Before we dive deeper into the behaviour of features using Data visualization, we will create functions for plotting to overcome tediosity\n",
    "- Scatter Plots (for Numerical Categories)\n",
    "- Box Plots (for Categorial Categories)\n",
    "- Count Plots (to see relative frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a89d91-ae66-46d4-9af6-ff281f7b8cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for scatter and box plots\n",
    "def plot_scat(col1, fig_size):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.title(f\"Scatter plot of {str(col1)} vs Sale Price\")\n",
    "    print(f'Correlation Coeff of {col1} and SalePrice: {round(np.corrcoef(df[col1], df[\"SalePrice\"])[0][1],5)}')\n",
    "    sns.scatterplot(x=col1, y='SalePrice', data=df);\n",
    "    \n",
    "def plot_box(col1, rot_title, fig_size):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.xticks(rotation=rot_title)\n",
    "    plt.title(f\"Sale Price of Houses by {str(col1)}\")\n",
    "    temp_df = df[[str(col1),'SalePrice']].sort_values(by='SalePrice', ascending=False)\n",
    "    sns.boxplot(x=col1, y='SalePrice', data=temp_df);\n",
    "    \n",
    "        \n",
    "def plot_count(col1, rot_title, fig_size):    \n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.xticks(rotation=rot_title)\n",
    "    plt.title(f\"Number of Houses Sold by {str(col1)}\")\n",
    "    temp_df = df[[str(col1),'SalePrice']].sort_values(by='SalePrice', ascending=False)\n",
    "    sns.countplot(x=col1, data=temp_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca1a03-ec65-48e1-a381-845eb9aa7a22",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3f. High Cardinality Feature - Neighborhood**</span>\n",
    "    \n",
    "The neighborhood could be influencial to a home’s value, because it is responsible for both qualitative and quantifiable aspects of a home’s appeal. For example, school quality or crime rate in the neighborhood significantly affects home values.<br>\n",
    "In our dataset, there are 29 different neighborhoods, and we can say that the neighborhood feature have high cardinality, which means there are many many of these unique values in a single column. We will expect One-Hot Encoding to be a problem in such a case if we have a separate column for each unique neighborhood. We will have too many unique values to model effectively, which can also lead to issues when training and testing our model. It’s possible that a neighborhood will show up in a test set, but not in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa4a07-6529-4cbd-8844-4f4cc27d1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box('Neighborhood', 90, (15,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba01b00-5867-4ee1-84f4-734e756b4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count(\"Neighborhood\", 90,(15.25,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5806e97-5bbc-49a3-88ed-0e0ebd8309b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data frame for Neighborhood, with -\n",
    "# mean = based on mean SalePrice for that Neighborhood\n",
    "# count = number of observations for that Neighborhood\n",
    "\n",
    "df_neigh = df.groupby('Neighborhood')['SalePrice'].agg(['mean', 'count'])\n",
    "df_neigh.sort_values(by='mean',ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d4d01-f2dc-4a7d-bc33-7f8c1792ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weak correlation coefficient between SalePrice and number of houses in the neighbourhood, \n",
    "# we will not take into account the number of houses in the neighbourhood\n",
    "np.corrcoef(df_neigh[\"mean\"], df_neigh[\"count\"])[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6582d7-4326-4a3c-95a9-cf078abec4aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want to group the 4 neighbourhood into 4 classes (neigh_class):\n",
    "\n",
    "# Class A: above 75th\n",
    "# Class B: above 50th percentile , 75th percentile & below\n",
    "# Class C: above 25th percentile , 50th percentile & below\n",
    "# Class D: 25th percentile & below\n",
    "\n",
    "for i in df_neigh.index:\n",
    "    if float(df_neigh[df_neigh.index == str(i)][\"mean\"]) > df_neigh['mean'].quantile(0.75):\n",
    "        df_neigh.loc[str(i),\"neigh_class\"] = 'A'\n",
    "    elif float(df_neigh[df_neigh.index == str(i)][\"mean\"]) <= df_neigh['mean'].quantile(0.25):\n",
    "        df_neigh.loc[str(i),\"neigh_class\"] = 'D'\n",
    "    elif float(df_neigh[df_neigh.index == str(i)][\"mean\"]) <= df_neigh['mean'].quantile(0.5):\n",
    "        df_neigh.loc[str(i),\"neigh_class\"] = 'C'\n",
    "    else:\n",
    "        df_neigh.loc[str(i),\"neigh_class\"] = 'B'\n",
    "\n",
    "df_neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e86199-ec69-4318-a3e9-1e46c6ef1503",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "neigh_class_col = []\n",
    "for i in df[\"Neighborhood\"]:\n",
    "    j = df_neigh.loc[i,\"neigh_class\"]\n",
    "    neigh_class_col.append(j)\n",
    "\n",
    "# Adding neighborhood class as a new feature on our dataframe\n",
    "df[\"neigh_class\"] = neigh_class_col\n",
    "df['neigh_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0402270d-25d7-4f40-be44-1ba7bd0c32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have grouped neighborhood into classes, \n",
    "# we can now drop our original neighborhood feature from our dataframe\n",
    "df.drop(labels=\"Neighborhood\", axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef1585-3375-47c8-b0c0-f4ba7e70c02f",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3g. Lot Area and Log Config**</span><br>\n",
    "Size is an important element to consider when predicting SalePrice, since a bigger house can positively impact its valuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c686d-a4d9-47e2-80ca-ef82888a1735",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_scat('Lot Area', (9,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde580f",
   "metadata": {},
   "source": [
    "We can see that there are some outliers for Lot Area (i.e. houses that have extremely huge lot area but reasonalble SalePrice). We want to remove these outliers to improve our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_99_percentile = np.quantile(df['Lot Area'], q=0.99)\n",
    "area_99_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e802e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Lot Area']>area_99_percentile].shape[0]\n",
    "# Remove Outliers - Dropping the houses with Lot Area > its 99th percentile area_99_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddcea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Lot Area']<=area_99_percentile]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9c679-d2e0-4259-9bbf-56f4116b98c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_box('Lot Config', 0, (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5ffe9-d801-4626-955d-63849037ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['Lot Config','Lot Area'], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb74b4-01cd-46b2-81ed-38a78f53a463",
   "metadata": {},
   "source": [
    "As we can see from the boxplots above, we can draw the following conclusions -<br>\n",
    "(i) Weak correlation between Lot Area and SalePrice <br>\n",
    "(ii) No clear distinction for medians and ranges of Lot Config <br>\n",
    "Based on (i) and (ii), no clear conclusions may be made $\\implies$ we have dropped the columns for `Lot Area` and `Lot Shape` <br><br>\n",
    "We want to do the same for **all** of the other columns.<br><br>\n",
    "<span style=\"color:royalblue\"> **3h. Dropping Features with Poor Relations**</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdec63-b04f-4b3d-9ca5-906a3c259729",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in df.drop('SalePrice',axis=1, inplace=False).columns:\n",
    "    if df[i].dtype == float and df[i].nunique()>30:\n",
    "        plot_scat(i, (5,5))\n",
    "    elif df[i].dtype == int and df[i].nunique()>30:\n",
    "        plot_scat(i, (5,5))\n",
    "    elif df[i].nunique()<=5:\n",
    "        plot_box(i, 0, (5,5))\n",
    "    elif df[i].nunique()<=15:\n",
    "        plot_box(i, 0, (7.5,5))\n",
    "    else:\n",
    "        plot_box(i, 90, (10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8f428",
   "metadata": {},
   "source": [
    "From the plots above, for the boxplotted features that no clear distinction $\\implies$ no clear conclusion may be made, so we will drop these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3c1d7-efe4-4d04-9e41-c520e44c68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=['Lot Shape','Land Slope','Bldg Type','Bsmt Full Bath','Half Bath',\n",
    "                'Bedroom AbvGr','Kitchen AbvGr','Mo Sold','Yr Sold'], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624beac8",
   "metadata": {},
   "source": [
    "For the numerical features that have poor correlation coefficient with sale price (i.e. $|r|<0.2$), we will drop these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63b8aa-fb2d-466c-a73d-dd6e3a42d9da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Printing all the correlation coefficient (w.r.t. SalePrice) for all the int/float columns\n",
    "low_corr = []\n",
    "for i in df.drop('SalePrice',axis=1, inplace=False).columns:\n",
    "    if df[i].dtype == float and df[i].nunique()>30:\n",
    "        print(i)\n",
    "        print(np.corrcoef(df[i], df[\"SalePrice\"])[0][1])\n",
    "        if abs(np.corrcoef(df[i], df[\"SalePrice\"])[0][1]) < 0.2:\n",
    "            low_corr.append(i)\n",
    "    elif df[i].dtype == int and df[i].nunique()>30:\n",
    "        print(i)\n",
    "        print(np.corrcoef(df[i], df[\"SalePrice\"])[0][1])\n",
    "        if abs(np.corrcoef(df[i], df[\"SalePrice\"])[0][1]) < 0.2:\n",
    "            low_corr.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a084e0-c021-4560-93e2-21da55b1c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Features with poor correlation coefficient (|r|<0.2):\\n {low_corr}')\n",
    "# these int/float columns have poor correlation coefficient with sale price (<0.2), we will drop these columns\n",
    "df.drop(labels=low_corr, axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad8b5c",
   "metadata": {},
   "source": [
    "**Heatmap of Numerical Features Correlation with SalePrice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d62b82-9a4c-4c32-bf55-5d1cc86470c6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "c = sns.heatmap(df.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613836b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cf5f0f",
   "metadata": {},
   "source": [
    "<span style=\"color:royalblue\"> **3i. Dummifying nominal features (One-hot-encoding)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d30e0b-574f-43cc-a06c-985aac675e18",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# forming a list of categorial columns with object values to dummify later\n",
    "lst_cat = [i for i in df.columns if df[i].dtype == object]\n",
    "# we also want to take into account categorial columns with integer/float values       \n",
    "lst_cat.append('MS SubClass')\n",
    "\n",
    "print(lst_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf107b9e-c48a-4d8f-9303-7a7a913d7fcb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now that we have the list of categorial columns, we want to dummify categorial columns\n",
    "dummy_df = pd.get_dummies(df,columns=lst_cat,drop_first=True)\n",
    "df = pd.concat([df,dummy_df],axis=1)\n",
    "df = df.loc[:,~df.columns.duplicated()]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc187af3-4df1-4917-92c9-fd82be902904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels=lst_cat, axis=1, inplace=True) # droping the columns that we have already dummify to avoid double countings\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0559f60-c0c5-4f71-813f-57cd46cb5d61",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bca6b1-bd03-4a5d-ac4e-4f7e36939a2b",
   "metadata": {},
   "source": [
    "### <span style=\"color:royalblue\"> **4. Model Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c6a1b-e5fd-4f7a-a42a-69a42c49a93c",
   "metadata": {},
   "source": [
    "#### <span style=\"color:royalblue\"> **4a. Train/Test/Split**\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e950c9-ee07-493b-9628-95688389e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(labels='SalePrice', axis=1, inplace=False)\n",
    "y = df['SalePrice']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db47679-0bdf-46dc-a311-bdb0639561fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdda765-25fc-4df1-8467-aa25a17a76b5",
   "metadata": {},
   "source": [
    "#### <span style=\"color:royalblue\"> **4b. Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6715691f-4eab-4c9a-96f8-95bd61239931",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbda5af-0afa-4b00-9765-e410ba06c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452c80d-2ba9-4374-905a-20df86f2dc5d",
   "metadata": {},
   "source": [
    "#### <span style=\"color:royalblue\"> **4c. Instantiate our Models**\n",
    "We want to evaluate 3 models: `LinearRegression`, `LassoCV` and `RidgeCV`, and use the one yields the best model metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07010b7-99f0-4370-a4d1-d94b111e37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lasso = LassoCV(n_alphas=200)\n",
    "ridge = RidgeCV(alphas=np.linspace(0.1, 10, 100))\n",
    "enet = ElasticNetCV(alphas=np.arange(0.5, 1.0, 0.005), l1_ratio=0.5, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef3b2c5-8126-4b74-8a50-4346e2d614d4",
   "metadata": {},
   "source": [
    "#### <span style=\"color:royalblue\"> **4d. Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2bb9b-0922-4b29-b5d2-46e75833b201",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we want to find out the optimal number of folds \n",
    "for i in range(2,6):\n",
    "    lr_scores = cross_val_score(lr, X_train, y_train, cv=i)\n",
    "    lasso_scores = cross_val_score(lasso, X_train, y_train, cv=i)\n",
    "    ridge_scores = cross_val_score(ridge, X_train, y_train, cv=i)\n",
    "    enet_scores = cross_val_score(enet, X_train, y_train, cv=i)\n",
    "    print(f'LinearRegression r2 score for {i}-fold validations: {lr_scores.mean()}')\n",
    "    print(f'Lasso r2 score for {i}-fold validations: {lasso_scores.mean()}')\n",
    "    print(f'Ridge r2 score for {i}-fold validations: {ridge_scores.mean()}')\n",
    "    print(f'ElasticNet r2 score for {i}-fold validations: {enet_scores.mean()}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44654172-653c-4023-a8cb-79647bdcdd2f",
   "metadata": {},
   "source": [
    "Based on the average $𝑅^2$ from across cross-validation evaluations, $cv=3$ gives the best $𝑅^2$ score, and we can conclude that 3-fold cross validations is optimal.<br>\n",
    "Comparing $𝑅^2$ between the different regression models with 3-fold cross validations on the training dataset - <br>\n",
    "- `LinearRegression` Mean $𝑅^2$  Score: $--3.63 \\times 10^{22}$<br>\n",
    "- `RidgeCV` Mean $𝑅^2$  Score: 0.907<br>\n",
    "- `LassoCV` Mean $𝑅^2$  Score: 0.902<br>\n",
    "- `ElasticNetCV` Mean $𝑅^2$  Score: 0.897<br>\n",
    "\n",
    "\n",
    "`LassoCV` have the best $𝑅^2$ Mean Score, we conclude that out of the 3 models, `Lasso` is the most suitable model for the training data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e27fe",
   "metadata": {},
   "source": [
    "#### <span style=\"color:royalblue\"> **4e. Model Fitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5e4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the average r^2 from across cv evaluations, we can conclude that cv=3 gives the best r2 score\n",
    "lasso = LassoCV(n_alphas=200)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lasso_alpha = lasso.alpha_\n",
    "best_lasso_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=best_lasso_alpha)\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffbe55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso.fit(X_train, y_train)\n",
    "# making predictions on testing set\n",
    "y_pred = lasso.predict(X_test)\n",
    "# comparing r2 score of model predicted y values vs true y values:\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Lasso R2 scoring fit model on training set: {lasso.score(X_train, y_train)}')\n",
    "print(f' Lasso R2 scoring fit model on testing set: {lasso.score(X_test, y_test)}')\n",
    "# Lasso model has lower bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f31ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Lasso model RMSE on training set: {np.sqrt(mean_squared_error(y_train, lasso.predict(X_train)))}')\n",
    "print(f' Lasso model RMSE on testing set: {np.sqrt(mean_squared_error(y_test, lasso.predict(X_test)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693889c",
   "metadata": {},
   "source": [
    "Our model's R2 score is close to 0.918 on our test data , which means our model explains around 91.8% of the variation of our test data's SalePrice. Since the training score (0.928) is only alightly higher than our testing score (0.918), there is little evidence of overfitting. We can agree that this is a decent model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a056f",
   "metadata": {},
   "source": [
    "#### <span style=\"color:royalblue\"> **4f. Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3a29d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso.coef_ # getting the regression coefficients for features (X) used in modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12f5d6-04cc-4480-898e-ba706c0e367f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_coeffs = {col:coeff for col, coeff in zip(X.columns, lasso.coef_)}\n",
    "model_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d15d07-d36a-401c-b1bd-0d5ecf28da86",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_coeffs_df = pd.DataFrame(model_coeffs, index=['coefficient']).T\n",
    "model_coeffs_df = model_coeffs_df.sort_values(by=['coefficient'], ascending=False)\n",
    "display(model_coeffs_df.head(10), model_coeffs_df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0805c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_coeffs_df_truncate = pd.concat([model_coeffs_df.head(10), model_coeffs_df.tail(10)])\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.barplot(data=model_coeffs_df_truncate, y=model_coeffs_df_truncate.index, x='coefficient', orient='h', palette=\"vlag\")\n",
    "plt.title('Top & Bottom 10 Features and its Lasso Coefficients');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca727ea",
   "metadata": {},
   "source": [
    "#### <span style=\"color:royalblue\"> **4g. Testing Linearity Assumptions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb798b1-a31d-4280-8e3a-9b8695ea4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_residuals = y_test - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4007013-ae0d-4cc9-8ffd-abbe7501a471",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.title('Model Predicted vs Actual SalePrice')\n",
    "plt.xlabel('Actual SalePrice')\n",
    "plt.ylabel('Predicted SalePrice')\n",
    "sns.regplot(x=y_test,y=y_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128942f7-2581-4571-9fe7-982210f7450f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "plt.title('Model Residuals vs Predicted SalePrice')\n",
    "plt.xlabel('Predicted SalePrice')\n",
    "plt.ylabel('Residuals')\n",
    "sns.regplot(x=y_pred, y=y_residuals);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d98f5ec-dcf2-41d4-a376-a0e08a6be62f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,3))\n",
    "plt.title('Regression Standardized Residuals for SalePrice')\n",
    "\n",
    "sns.distplot(y_residuals, bins = 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f333b8",
   "metadata": {},
   "source": [
    "While the skew value indicates a considerable amount of right skewness, there appears to be no discernible pattern in the residuals. Hence, we can consider the (LINE) assumptions satisfied."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "858069e0",
   "metadata": {},
   "source": [
    "### <span style=\"color:royalblue\"> **5. Conclusions**\n",
    "#### <span style=\"color:royalblue\"> **5a. In comparison with our Baseline Model**\n",
    "    \n",
    "The Lasso model we have designed made a significant improvement as compared to the Baseline Model:\n",
    "\n",
    "\n",
    "| Model: | Baseline | Lasso |\n",
    "| --- | --- | --- | \n",
    "| $R^2 $ score | 0.840 | 0.9183 |\n",
    "| RMSE | 31609 | 21195 |\n",
    "\n",
    "\n",
    "This addresses our problem statement of building a model to accurately predict housing prices.\n",
    "\n",
    "\n",
    "#### <span style=\"color:royalblue\"> **5b. Recommendations**\n",
    "\n",
    "Perhaps, knowing the factors that will influence the house price may more valuable than the price predictions.\n",
    "    \n",
    "10 unique house features might be **positively correlated** with our house prices:\n",
    "    \n",
    "- Above Grade (ground) Living Area (square feet)\n",
    "- Overall Quality \n",
    "- Basement (Type 1) Finished Area (square feet)\n",
    "- Garage Area\n",
    "- Overall Condition\n",
    "- Typical Functionality \n",
    "- Brick Face Exterior Covering on house\n",
    "- Number of Fireplaces\n",
    "- Good Basement Exposure\n",
    "\n",
    "We found that the top 3 features positively correlated with SalePrice are Ground Living Area, Overall Quality, as well as Basement Finished Area. In other words, these features, when present in greater magnitude, are likely to produce higher SalePrice. For example, a house with a higher Overall Quality would likely fetch a higher price than a house with a lower overall Quality. \n",
    "\n",
    "Homeowners may be more inclined to improve on the housing features that are generally amenable. E.g.:\n",
    "\n",
    "- Select a better material and finish of the house to improve Overall Quality\n",
    "- Basement (Type 1) Finished Area (square feet)\n",
    "- Have a larger garage space (area)\n",
    "    \n",
    "\n",
    "5 unique house features might be **negatively correlated** with our house prices:\n",
    "- AgeSold (in Years)\n",
    "- External Quality Material: Average/Typical\n",
    "- Kitchen Quality: Average/Typical\n",
    "- Kitchen Quality: Good\n",
    "- Class D Neighborhood\n",
    "\n",
    "In other words, according to the observations drawn from the dataset, houses with such features are likely to fetch lower prices. \n",
    "\n",
    "While Age and Class D Neighborhood are unsurprising, the External Quality & Kitchen Quality are somewhat unusual. It is plausible that Ames’ citizens (for some reason) emphasize greatly on the house’s exterior material and kitchens, the category of Average/Typical is not expected. This implies that even houses with poor Exterior material or kitchen quality might somehow fetch higher prices than houses with average exterior material or kitchen, which does not make logical sense. A possible reason for this phenomenon could be that given the small size of our dataset (which only has slightly more than 2k+ houses), we do not have sufficient data points in the other categories to verify the correlation.\n",
    "\n",
    "Two-cents worth for homebuyers:\n",
    "\n",
    "- Avoid old houses in class D neighborhoods\n",
    "- Buy houses that are built not too long ago, since prices deteriorate with age\n",
    "\n",
    "\n",
    "\n",
    "#### <span style=\"color:royalblue\"> **5c. Model Caveats**\n",
    "\n",
    "\n",
    "Sensitive to context; Our model is unlikely to be generalizable to other cities. The features we chose to build the model might not be the most ideal selection to determine house prices, because we did not have enough domain knowledge to perform manual feature selection.  In reality, two populations in different cities or countries may not share the same considerations for desirable features in a house. \n",
    "\n",
    "To make the model more generalizable, we could cut down on the number of variables and use only general features found and assessed within most houses in the world, such as distance to a transport system. This would of course come at an expense of the predictive power of the model.\n",
    "\n",
    "\n",
    "#### <span style=\"color:royalblue\"> **5d. Possible Enhancements**\n",
    " \n",
    "- Testing other predictive models such as random forest, support vector machines, since we are limited to only linear  regression here\n",
    "- Obtaining more housing data, preferably recent, to update the model and expose it to more variations\n",
    "- Obtaining more general features of the house such as crime rate, ethnicity, distance to amenities or facilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16545ef3",
   "metadata": {},
   "source": [
    "### <span style=\"color:royalblue\"> **Annex (Kaggle Submission)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407754bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.set_index('Id', inplace=True)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73996d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning all in one function \n",
    "\n",
    "def clean_test_set(data):\n",
    "    \n",
    "    data['AgeSold'] = data['Yr Sold'] - data['Year Built']\n",
    "    data['AgeRemod'] = data['Yr Sold'] - data['Year Remod/Add']\n",
    "    data[data['AgeSold']<0] = 0\n",
    "    data[data['AgeRemod']<0] = 0\n",
    "    data.drop(labels=['Year Built','Year Remod/Add'], axis=1, inplace=True)\n",
    "\n",
    "    for i in data.columns:\n",
    "        if data[i].dtypes == object:\n",
    "            data[i].replace(np.nan, \"NA\",inplace=True)\n",
    "    \n",
    "    data['Bsmt Qual'] = data['Bsmt Qual'].fillna('NA')\n",
    "    data.loc[data['Bsmt Qual']=='NA','Bsmt Full Bath'] = 0\n",
    "    data.loc[data['Bsmt Qual']=='NA','Bsmt Half Bath'] = 0\n",
    "    data.loc[data['Bsmt Qual']=='NA','BsmtFin SF 1'] = 0\n",
    "    data.loc[data['Bsmt Qual']=='NA','BsmtFin SF 2'] = 0\n",
    "    data.loc[data['Bsmt Qual']=='NA','Bsmt Unf SF'] = 0\n",
    "    data.loc[data['Bsmt Qual']=='NA','Total Bsmt SF'] = 0\n",
    "\n",
    "    data.loc[data['Garage Finish']=='NA','Garage Yr Blt'] = 0\n",
    "    \n",
    "    test_gar = data.filter(regex= 'Garage')\n",
    "    test_gar_null = test_gar[test_gar['Garage Area'].isnull()]\n",
    "    \n",
    "    for i in list(test_gar_null.index):\n",
    "        data.loc[i , 'Garage Yr Blt'] = round(df_gar['Garage Yr Blt'].mean(),0)\n",
    "        data.loc[i , 'Garage Cars'] = round(df_gar['Garage Cars'].mean(),0)\n",
    "        data.loc[i , 'Garage Area'] = round(df_gar['Garage Area'].mean(),0)\n",
    "        data.loc[i , 'Garage Finish'] = df_gar['Garage Finish'].mode()[0]\n",
    "        data.loc[i , 'Garage Qual'] = df_gar['Garage Qual'].mode()[0]\n",
    "        data.loc[i , 'Garage Cond'] = df_gar['Garage Cond'].mode()[0]\n",
    "        \n",
    "    data['Mas Vnr Type'].replace('NA',None, inplace=True) \n",
    "    data.loc[(data['Mas Vnr Type']=='BrkFace') & (data['Mas Vnr Area'].isnull()),'Mas Vnr Area'] = mean_mas_vnr_brk\n",
    "    data.loc[(data['Mas Vnr Type']=='Stone') & (data['Mas Vnr Area'].isnull()),'Mas Vnr Area'] = mean_mas_vnr_stone\n",
    "    \n",
    "    neigh_class_col = []\n",
    "    for i in data[\"Neighborhood\"]:\n",
    "        j = df_neigh.loc[i,\"neigh_class\"]\n",
    "        neigh_class_col.append(j)\n",
    "    \n",
    "    data[\"neigh_class\"] = neigh_class_col\n",
    "    \n",
    "    \n",
    "    data.drop(['Neighborhood','PID', 'Sale Type','Lot Frontage','Street','Alley','Utilities', 'Condition 2', 'Roof Matl',\n",
    "         'Low Qual Fin SF', 'Heating', 'Pool QC', 'Pool Area', 'Misc Feature','Misc Val', '3Ssn Porch','Lot Config',\n",
    "         'Lot Shape','Land Slope','Bldg Type','Bsmt Full Bath','Half Bath','Bedroom AbvGr','Kitchen AbvGr',\n",
    "         'Misc Val','Mo Sold','Yr Sold','BsmtFin SF 1', 'BsmtFin SF 2', 'Bsmt Unf SF', '2nd Flr SF', \n",
    "         'Garage Yr Blt', 'Wood Deck SF', 'Open Porch SF', 'Enclosed Porch', 'Screen Porch'], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    dummy_df = pd.get_dummies(data,columns=lst_cat, drop_first=True)\n",
    "    data = pd.concat([data,dummy_df],axis=1)\n",
    "    data = data.loc[:,~data.columns.duplicated()]\n",
    "    data.drop(labels=lst_cat, axis=1, inplace=True)\n",
    "    \n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dcc719-9d59-4ace-a2f3-e3175adab1ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df  = clean_test_set(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf970eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns in test but not in train\n",
    "not_in_train = []\n",
    "for i in test_df.columns:\n",
    "    if i not in df.columns:\n",
    "        not_in_train.append(i)\n",
    "print(not_in_train)\n",
    "#we want to drop these columns\n",
    "test_df.drop(labels = not_in_train, axis=1, inplace=True)\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83759d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a14248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns in train but not in test\n",
    "not_in_test = []\n",
    "for i in df.drop(labels='SalePrice',axis=1,inplace=False).columns:\n",
    "    if i not in test_df.columns:\n",
    "        not_in_test.append(i)\n",
    "        \n",
    "print(not_in_test)\n",
    "\n",
    "#we want to add these missing columns into our test and assign value 0\n",
    "for j in not_in_test:\n",
    "    test_df[j] = 0\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_test = ss.transform(test_df)\n",
    "pred_test_y = lasso.predict(scale_test)\n",
    "test_df['SalePrice'] = pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_col = test_df.pop(\"SalePrice\")\n",
    "test_df.insert(0, 'SalePrice', sp_col)\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df = test_df[['Id','SalePrice']]\n",
    "kaggle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a19897-48c3-4147-ad01-622f154a9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_df.to_csv('./datasets/kaggle_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d157be",
   "metadata": {},
   "source": [
    "*Thank you!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
